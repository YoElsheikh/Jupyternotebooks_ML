{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5603d69",
   "metadata": {},
   "source": [
    "## Calculating LSTM Gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0506fd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 12],\n",
       "       [21, 32]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we assume that there's a softmax smoothing at the cell output\n",
    "import numpy as np\n",
    "np.multiply(a,b)\n",
    "\n",
    "#f_t = sigmoid function at the forget gate\n",
    "#i_t = sigmoid function at the input\n",
    "#o_t = sigmoid function at the output gate\n",
    "\n",
    "#Wf, bf = weights and bises at the forget gate\n",
    "#Wi, bi = weights and bises at the input gate\n",
    "#Wc, bc = weights and bises at the tanh (cell) gate\n",
    "#Wo, bo = weights and bises at the output gate\n",
    "#Wy, by = weights and bises at the output (softmax)\n",
    "\n",
    "Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "f_t = sigmoid(h_prev @ Wf + bf)\n",
    "i_t = sigmoid(h_prev @ Wi + bi)\n",
    "o_t = sigmoid(h_prev @ Wo + bo)\n",
    "c_tanh = tanh(h_prev @ Wc + bc) \n",
    "\n",
    "    c = f_t * c_old + hi * hc\n",
    "    h = o_t * tanh(c)\n",
    "\n",
    "    y = h @ Wy + by\n",
    "    output = softmax(y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient between hidden and y\n",
    "    dWy = h.T @ dy\n",
    "    dby = dy\n",
    "#NOTE: the gradient of the next h should be added \n",
    "\n",
    "    # sigmoid function @ h = o_i * tanh(c)\n",
    "    do_i = tanh(c) * dh\n",
    "    do_i = dsigmoid(o_i) * do_i\n",
    "\n",
    "    #cell state @ h = o_i * tanh(c), note we're adding dc_next here\n",
    "    dc = o_i * dh * dtanh(c)\n",
    "    #dc = dc + dc_next\n",
    "\n",
    "    #sigmoid function at the forget gate @ c = hf * c_old + i_t * c_tahn\n",
    "    df_i = c_old * dc\n",
    "    df_i = dsigmoid(f_i) * dhf\n",
    "\n",
    "    #sigmoid function at the input gate @ c = f_t * c_old + i_t * c_tanh\n",
    "    di_t = c_tanh * dc\n",
    "    di_t = dsigmoid(i_t) * di_t\n",
    "\n",
    "    # tan hyperbolic gate (c_tanh) @ c = f_t * c_old + i_t * c_tanh\n",
    "    dc_tanh = i_t * dc\n",
    "    dc_tanh = dtanh(c_tanh) * dc_tanh\n",
    "\n",
    "    \n",
    "    #h_prev is the concatenated hidden state\n",
    "    dWf = h_prev.T @ df_t\n",
    "    dbf = df_t\n",
    "    dh_prev_f = dhf @ Wf.T\n",
    "\n",
    "    dWi = h_prev.T @ di_t\n",
    "    dbi = di_t\n",
    "    dh_prev_i = di_t @ Wi.T\n",
    "\n",
    "    dWo = h_prev.T @ do_t\n",
    "    dbo = do_t\n",
    "    dh_prev_o = do_t @ Wo.T\n",
    "\n",
    "    dWc = h_prev.T @ dc_tanh\n",
    "    dbc = dc_tanh\n",
    "    dh_prev_c_tanh = dc_tanh @ Wc.T\n",
    "\n",
    "    # Concatinating the gradients \n",
    "    dh_prev = dh_prev_o + dh_prev_c_tanh + dh_prev_i + dh_prev_f\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
